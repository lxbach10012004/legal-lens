{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import torch\n","import transformers\n","from datasets import load_dataset\n","from torch.utils.data import DataLoader, Dataset\n","from tqdm import tqdm\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T13:20:20.161598Z","iopub.status.busy":"2024-08-13T13:20:20.161078Z","iopub.status.idle":"2024-08-13T13:20:20.218033Z","shell.execute_reply":"2024-08-13T13:20:20.216853Z","shell.execute_reply.started":"2024-08-13T13:20:20.16157Z"},"trusted":true},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["access_token = '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'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T13:20:20.220463Z","iopub.status.busy":"2024-08-13T13:20:20.219623Z","iopub.status.idle":"2024-08-13T13:20:23.168463Z","shell.execute_reply":"2024-08-13T13:20:23.167549Z","shell.execute_reply.started":"2024-08-13T13:20:20.220418Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"huudung141434/deberta-legal-nli\", token=access_token)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T13:20:23.171881Z","iopub.status.busy":"2024-08-13T13:20:23.171529Z","iopub.status.idle":"2024-08-13T13:20:23.321689Z","shell.execute_reply":"2024-08-13T13:20:23.320534Z","shell.execute_reply.started":"2024-08-13T13:20:23.171854Z"},"trusted":true},"outputs":[],"source":["tdf = pd.read_excel('testset_NLI_LegalLens.xlsx')\n","tdf = tdf.assign(label='None')\n","tdf.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T13:20:23.323864Z","iopub.status.busy":"2024-08-13T13:20:23.32313Z","iopub.status.idle":"2024-08-13T13:20:23.329238Z","shell.execute_reply":"2024-08-13T13:20:23.328209Z","shell.execute_reply.started":"2024-08-13T13:20:23.323822Z"},"trusted":true},"outputs":[],"source":["label_mapping = {0 : 'Contradict', 1 : 'Entailed', 2 : 'Neutral'}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T13:20:23.330929Z","iopub.status.busy":"2024-08-13T13:20:23.33054Z","iopub.status.idle":"2024-08-13T13:20:23.340325Z","shell.execute_reply":"2024-08-13T13:20:23.33916Z","shell.execute_reply.started":"2024-08-13T13:20:23.330904Z"},"trusted":true},"outputs":[],"source":["class LegalLensDataset(Dataset):\n","    def __init__(self, data, tokenizer, max_len = 512, num_labels = 3):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","    def __getitem__(self, idx):\n","        try:\n","            item = self.data.iloc[idx]\n","            pre = item.premise\n","            hypo = item.hypothesis\n","            encoded_input = self.tokenizer(pre, hypo, padding = 'max_length',\n","                                truncation = True, max_length = self.max_len, \n","                                           return_tensors = 'pt')\n","            return {'input_ids' : encoded_input['input_ids'].squeeze(),\n","                  'token_type_ids' : encoded_input['token_type_ids'].squeeze(),\n","                  'attention_mask' : encoded_input['attention_mask'].squeeze()}\n","        except:\n","            traceback.print_exc()\n","            return None\n","    def __len__(self):\n","        return len(self.data)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T13:20:23.358141Z","iopub.status.busy":"2024-08-13T13:20:23.357823Z","iopub.status.idle":"2024-08-13T13:20:23.367688Z","shell.execute_reply":"2024-08-13T13:20:23.366529Z","shell.execute_reply.started":"2024-08-13T13:20:23.358115Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","def infer_deberta(model, tdf, dataset, device):\n","    df = tdf.copy()\n","    model = model.to(device)\n","    model.eval()\n","    all_preds = []\n","    total_samples = 0\n","    for item in tqdm(dataset):\n","        premise = item['premise']\n","        hypothesis = item['hypothesis']\n","        inputs = tokenizer(premise, hypothesis, padding = 'max_length',truncation = True, max_length = 512, return_tensors = 'pt')\n","        inputs = inputs.to(device)\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        prediction = torch.argmax(logits, dim=-1)\n","        all_preds.append(prediction.item())\n","    for i in range(len(df)):\n","        df.label.at[i] = label_mapping[all_preds[i]]\n","    return df\n","        \n","def save_output(df, name):\n","    df.to_csv(f'{name}.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T13:20:23.38071Z","iopub.status.busy":"2024-08-13T13:20:23.380271Z","iopub.status.idle":"2024-08-13T13:20:23.409878Z","shell.execute_reply":"2024-08-13T13:20:23.408836Z","shell.execute_reply.started":"2024-08-13T13:20:23.380678Z"},"trusted":true},"outputs":[],"source":["from datasets import Dataset\n","test_dataset = Dataset.from_pandas(tdf)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T13:20:23.411405Z","iopub.status.busy":"2024-08-13T13:20:23.411098Z","iopub.status.idle":"2024-08-13T13:20:38.12296Z","shell.execute_reply":"2024-08-13T13:20:38.1219Z","shell.execute_reply.started":"2024-08-13T13:20:23.411379Z"},"trusted":true},"outputs":[],"source":["out_df = infer_deberta(model, tdf, test_dataset, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T13:20:51.745268Z","iopub.status.busy":"2024-08-13T13:20:51.744962Z","iopub.status.idle":"2024-08-13T13:20:51.756714Z","shell.execute_reply":"2024-08-13T13:20:51.755799Z","shell.execute_reply.started":"2024-08-13T13:20:51.745242Z"},"trusted":true},"outputs":[],"source":["out_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out_df.to_csv('predictions_NLILens.csv', index = False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def check_nli_format(predictions_file_path, test_file_path):\n","    \"\"\"\n","    Check the format of the NLI prediction file.\n","    The file should be in CSV format with columns: Premise, hypothesis, label\n","    \"\"\"\n","    try:\n","        df = pd.read_csv(predictions_file_path)\n","    except Exception as e:\n","        return False, f\"Error reading predictions CSV file: {e}\"\n","    \n","    try:\n","        test_df = pd.read_csv(test_file_path)\n","    except Exception as e:\n","        return False, f\"Error reading test CSV file: {e}\"\n","    \n","    # Check expected columns\n","    expected_columns = ['premise', 'hypothesis', 'label']\n","    pred_columns = list(df.columns)\n","    for expected_col in expected_columns:\n","        if expected_col not in pred_columns:\n","            return False, f\"Incorrect columns. Expected: {expected_columns}, Found: {pred_columns}\"\n","    \n","    # Check number of rows\n","    expected_nli_num_rows = len(test_df)\n","    predictions_nli_num_rows = len(df)\n","    if predictions_nli_num_rows != expected_nli_num_rows:\n","        return False, f\"Incorrect number of predictions. Expected: {expected_nli_num_rows}, Found: {predictions_nli_num_rows}\"\n","    \n","    return True, \"NLI prediction file format is correct.\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["is_valid, message = check_nli_format('predictions_NLILens.csv', 'testset_NLI_LegalLens.csv')\n","print(f\"NLI File Check: {message}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5459824,"sourceId":9055220,"sourceType":"datasetVersion"},{"datasetId":5533296,"sourceId":9159457,"sourceType":"datasetVersion"},{"datasetId":5511146,"sourceId":9162427,"sourceType":"datasetVersion"},{"datasetId":5536705,"sourceId":9163826,"sourceType":"datasetVersion"},{"datasetId":5536852,"sourceId":9164036,"sourceType":"datasetVersion"},{"datasetId":5537413,"sourceId":9164760,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
